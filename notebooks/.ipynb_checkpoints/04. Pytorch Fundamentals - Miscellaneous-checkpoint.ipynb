{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors & Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy is generally used to express data in scientific computation and PyTorch has its own functionality to interact with NumPy.\n",
    "* Data in NumPy, want to convert into PyTorch Tensor --> `torch.from_numpy(ndarray)`\n",
    "* Data in Tensor, want to convert into Numpy Array --> `torch.Tensor.numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor --> Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Array:\t[1. 2. 3. 4. 5. 6. 7.]\n",
      "Torch Tensor:\ttensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "# Create an array in Numpy\n",
    "array = np.arange(1.,8.)\n",
    "tensor = torch.from_numpy(array)\n",
    "\n",
    "print(f\"Numpy Array:\\t{array}\\nTorch Tensor:\\t{tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy --> Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Tensor:\ttensor([1., 1., 1., 1., 1.])\n",
      "Numpy Array:\t[1. 1. 1. 1. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(5)\n",
    "array = tensor.numpy()\n",
    "\n",
    "print(f\"Torch Tensor:\\t{tensor}\\nNumpy Array:\\t{array}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **Note For Use** **<br>\n",
    "The default dtype of tensor is `float64` and the default dtype of numpy is `float32`. So, while converting them, please make sure which dtype are you going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Array:\t(array([1., 1., 1., 1., 1.], dtype=float32), dtype('float32'))\n",
      "Torch Tensor:\t(tensor([1., 1., 1., 1., 1.]), torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Let's reconvert the numpy array to a tensor\n",
    "tensor = torch.from_numpy(array)\n",
    "array = tensor.numpy()\n",
    "\n",
    "print(f\"Numpy Array:\\t{array, array.dtype}\\nTorch Tensor:\\t{tensor, tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Reproducibility\n",
    "To reuce the randomness of random tensors using `RANDOM_SEED`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A:\n",
      "tensor([[0.7376, 0.5446, 0.7413, 0.0991],\n",
      "        [0.1063, 0.9697, 0.3540, 0.7472],\n",
      "        [0.5134, 0.6461, 0.7845, 0.6066],\n",
      "        [0.2953, 0.3178, 0.9595, 0.2324]])\n",
      "Tensor B:\n",
      "tensor([[0.9052, 0.3821, 0.2105, 0.4362],\n",
      "        [0.8799, 0.8265, 0.9341, 0.3529],\n",
      "        [0.3267, 0.0795, 0.4248, 0.0141],\n",
      "        [0.7809, 0.1505, 0.3953, 0.3395]])\n",
      "Check Equality:\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "tensorA = torch.rand(4,4)\n",
    "tensorB = torch.rand(4,4)\n",
    "\n",
    "print(f\"Tensor A:\\n{tensorA}\\nTensor B:\\n{tensorB}\\nCheck Equality:\")\n",
    "print(tensorA == tensorB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With normal tensors without `random seed` it shows complete randomness while producing random tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor C:\n",
      "tensor([[0.2961, 0.5166, 0.2517, 0.6886],\n",
      "        [0.0740, 0.8665, 0.1366, 0.1025],\n",
      "        [0.1841, 0.7264, 0.3153, 0.6871],\n",
      "        [0.0756, 0.1966, 0.3164, 0.4017]])\n",
      "Tensor D:\n",
      "tensor([[0.2961, 0.5166, 0.2517, 0.6886],\n",
      "        [0.0740, 0.8665, 0.1366, 0.1025],\n",
      "        [0.1841, 0.7264, 0.3153, 0.6871],\n",
      "        [0.0756, 0.1966, 0.3164, 0.4017]])\n",
      "Check Equality:\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "tensorC = torch.rand(4,4)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "tensorD = torch.rand(4,4)\n",
    "\n",
    "print(f\"Tensor C:\\n{tensorC}\\nTensor D:\\n{tensorD}\\nCheck Equality:\")\n",
    "print(tensorC == tensorD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Tensors on GPUs\n",
    "\n",
    "GPUs make faster and lighter computations on numbers by using `CUDA` with `NVIDIA` hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU Access check\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:\tcuda\n"
     ]
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device:\\t{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of active GPUs\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\ttensor([1, 2, 3])\n",
      "Device:\tcpu\n"
     ]
    }
   ],
   "source": [
    "# Putting tensors (and models) on the GPU\n",
    "import torch\n",
    "tensor = torch.tensor([1,2,3]) # By default device is on CPU\n",
    "print(f\"Tensor:\\t{tensor}\\nDevice:\\t{tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\ttensor([1, 2, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Move tensor to GPU (if available)\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "print(f\"Tensor:\\t{tensor_on_gpu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving tensors back to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tensors cannot transform tensors on GPUs, therefore error will occur\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtensor_on_gpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# Tensors cannot transform tensors on GPUs, therefore error will occur\n",
    "tensor_on_gpu.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor on CPU in Numpy: \t[1 2 3]\n",
      "Tensor on GPU in PyTorch:\ttensor([1, 2, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# To fix the GPU tensor with NumPy issue, we can set it to the CPU\n",
    "\n",
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "print(f\"Tensor on CPU in Numpy: \\t{tensor_back_on_cpu}\")\n",
    "print(f\"Tensor on GPU in PyTorch:\\t{tensor_on_gpu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU vs CPU Ì¶ Efficiency Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Device\t: cpu\n",
      "Active Device\t: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Active Device\\t: {device}\")\n",
    "\n",
    "# Define a tensor\n",
    "dim = 1000\n",
    "x_cpu = torch.randn(dim, dim)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Active Device\\t: {device}\")\n",
    "    x_gpu = x_cpu.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU calculation\n",
    "start_cpu = time.time()\n",
    "for _ in range(1000):\n",
    "    y_cpu = x_cpu.matmul(x_cpu)\n",
    "end_cpu = time.time()\n",
    "cpu_time = end_cpu - start_cpu\n",
    "\n",
    "# GPU Calculation\n",
    "start_gpu = time.time()\n",
    "for _ in range(1000):\n",
    "    y_gpu = x_gpu.matmul(x_gpu)\n",
    "end_gpu = time.time()\n",
    "gpu_time = end_gpu - start_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time\t: 5.8481 seconds\n",
      "GPU Time\t: 0.2840 seconds\n",
      "Speedup\t\t: 20.59x faster!\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(f\"CPU Time\\t: {cpu_time:.4f} seconds\")\n",
    "print(f\"GPU Time\\t: {gpu_time:.4f} seconds\")\n",
    "print(f\"Speedup\\t\\t: {cpu_time / gpu_time:.2f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">--< THE END >--</h1> \n",
    "@MUBA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (CUDA)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
