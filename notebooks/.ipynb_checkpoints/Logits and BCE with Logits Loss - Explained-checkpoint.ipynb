{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15eaea3",
   "metadata": {},
   "source": [
    "# Logits, BCE Loss, and BCE with Logits Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ddaf8",
   "metadata": {},
   "source": [
    "Binary classification is a fundamental task in machine learning, often using the Binary Cross-Entropy (BCE) loss function combined with a Sigmoid activation. However, an alternative, BCEWithLogitsLoss, provides better numerical stability and is preferred in many cases.\n",
    "\n",
    "To make this topic easier to understand, I will explain it in two ways:\n",
    "\n",
    "1. **A simple, non-technical manner**\n",
    "2. **A technical and mathematically reasoned approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0b6b7",
   "metadata": {},
   "source": [
    "## Logits and BCE Loss: Simple Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26857e1a",
   "metadata": {},
   "source": [
    "### What are Logits?\n",
    "\n",
    "Logits are the raw outputs of a neural network before applying an activation function like Sigmoid or Softmax. Unlike probabilities (which range from 0 to 1), logits can take any real value (−∞,+∞).\n",
    "\n",
    "\n",
    "### How Logits Relate to BCE Loss?\n",
    "\n",
    "We normally have multiple parameters inside an NN architecture with multiple different types of layers (e.g., weights & bias) connceted to one another. While we train our models to learn to adjust the parameters, we send the parameters with a forward pass function where we use the activation function like sigmoid. The activation function is nothing but a normalizing function that represents a raw linear combination of inputs and learned weights (**logits**) into a probabilistic range (0 to 1). \n",
    "\n",
    "So, when we use the BCE loss function, we technically apply the sigmoid function first to the final layer before calculating the loss in backpropagation. This means, when using BCE loss for a binary classification task, we must include a sigmoid activation function in the final layer to represent the logits into a range of probability. \n",
    "\n",
    "\n",
    "### Why do we apply Sigmoid?\n",
    "\n",
    "But what will happen if we do not use an activation function? Well, the model's network could output values outside the normal range [0, 1] of probability and lead to negative logs while calculating loss while causing instability with gradients.\n",
    "\n",
    "\n",
    "### Why use Logits?\n",
    "\n",
    "To simply put, logits are more expressive than a restricted range of probability [0, 1]. Logits provide an unrestricted range (−∞,+∞), meaning a more expressive range of a parameter's learning pattern rather than a restricted range of (0, 1). Eventually, this avoids saturation later when we use sigmoid activation.\n",
    "\n",
    "\n",
    "### How  are Logits useful?\n",
    "\n",
    "We already covered the first usefulness of logits above; now let's see how it helps future sigmoid activation later on.\n",
    "\n",
    "If we apply sigmoid activation before training (what we do in BCE Loss), we might face gradient vanishing issues (for more details on vanishing gradients, <a href=\"https://www.engati.com/glossary/vanishing-gradient-problem\">click here</a>) when the network produces very high or very low values. For example:\n",
    "\n",
    "*If the logits range is very large, let's say (-100, 100), then the sigmoid function saturates to 0 or 1, leading to a near-zero gradient. Which by all means is a bad thing for our model. This leads to the computational instability with gradients. Because computing BCE loss directly with probabilities can lead to an undefined value or undefined results beyond the probability range. So, we could say logits are important for **three things** mainly:*\n",
    "\n",
    "* Logits are more expressive.\n",
    "* Logits help avoid vanishing gradients.\n",
    "* Logits are computationally stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23852e2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcc0d598",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e73125c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "111e6be5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f80b9ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e418753",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c906aa6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af408f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a287eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c27ebc54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02f3c5fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70849a01",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (CUDA)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
