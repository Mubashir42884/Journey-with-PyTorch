{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e32a21",
   "metadata": {},
   "source": [
    "# **PYTORCH CV - CONVULUTIONAL NEURAL NETS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ab0a0",
   "metadata": {},
   "source": [
    "### What is Convolution?\n",
    "\n",
    "Convolution is a mathematical operation used to **combine two functions (or data) to produce a third function** that shows how one function modifies the other.\n",
    "\n",
    "In image processing, convolution is commonly used to extract features (like edges, textures, shapes, or patterns) by applying a filter (a.k.a kernel) to the image.\n",
    "\n",
    "* ***For example:*** *A 3x3 filter (e.g., edge detection filter or kernel) slides over the image (matrix data) and multiplies the filter values with the image pixel values (matrix values), summing them up to create a new value. This is repeated across the entire image (matrix data) to create a **\"feature map\"** of the image (matrix data).*\n",
    "\n",
    "\n",
    "Let's see a detailed calculation animation of convolution operation:\n",
    "\n",
    "<img src=\"../resources/Convolution_Operation.gif\" width=50%></img>\n",
    "*A 6x6 matrix, representing an image data matrix, is being convolved with a 3x3 filter (kernel) using a step size of 3. This means that during each iteration, the filter shifts 3 steps horizontally and vertically across the matrix. Typically, the step size is set to 1, allowing the filter to move only one step at a time.*\n",
    "\n",
    "</br>\n",
    "\n",
    "<font color=\"salmon\"><b><i>⭑Note: A 6x6 matrix convolved with a 3x3 filter can produce outputs of different dimensions depending on the step size (stride). If the step size is 1, the output will be a 4x4 matrix. With a step size of 2, the output will be a 2x2 matrix. A step size greater than 2 will not allow the 3x3 filter to fit within the 6x6 matrix, making convolution impossible.</i></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245c7a4",
   "metadata": {},
   "source": [
    "### How Do Convolutional Neural Networks (CNNs) Work?\n",
    "\n",
    "CNNs are specialized neural networks designed for processing structured data like images. They work in the following steps:\n",
    "1. **Convolution Layer:**\n",
    "    * Filters (small matrices) slide over the input image and extract features like edges, corners, or textures. This creates feature maps.\n",
    "    * Example: A 3x3 edge-detection filter highlights edges in an image.\n",
    "\n",
    "\n",
    "2. **ReLU Activation:**\n",
    "    * Applies a non-linear function $ReLU = max(0, x)$ to the feature maps to introduce non-linearity and remove negative values.\n",
    "\n",
    "\n",
    "3. **Pooling (Downsampling):**\n",
    "    * Reduces the size of feature maps while retaining important features (e.g., MaxPooling takes the maximum value in a region).\n",
    "    * Example: A 2x2 pooling layer reduces the size of a 4x4 feature map to 2x2.\n",
    "    * Types of Pooling:\n",
    "        - $Max Pooling (MaxPool)$: Takes the maximum value in each pooling region (e.g., 2x2 or 3x3) and retains the most prominent (strongest) feature in the region.\n",
    "        - $Average Pooling (AvgPool)$: Calculates the average value of all elements in the pooling region and preserves smoother feature representations by averaging.\n",
    "        - $Min Pooling (MinPool)$: Takes the minimum value in each pooling region and captures subtle or weak features by highlighting the smallest value.\n",
    "\n",
    "\n",
    "4. **Fully Connected Layer:**\n",
    "    * Flattened feature maps are passed to traditional dense layers for final classification or regression tasks.\n",
    "\n",
    "\n",
    "5. **Softmax/Output Layer:**\n",
    "    * Converts final outputs into probabilities or predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db90ed62",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"> What Makes CNN Special? </h3>\n",
    "\n",
    "##### Automatic Feature Extraction\n",
    "CNNs learn important features (edges, patterns) from data automatically, unlike traditional models where features are manually engineered.\n",
    "##### Translation Invariance\n",
    "CNNs recognize patterns regardless of their location in the input (e.g., a face in an image can be detected whether it's in the center or corner).\n",
    "##### Parameter Efficiency\n",
    "CNNs use fewer parameters due to shared weights in filters, making them computationally efficient.\n",
    "##### Great for Images and Spatial Data\n",
    "CNNs excel at capturing spatial hierarchies (edges → shapes → objects) in data like images, videos, or even audio spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a86fb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569fd016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ade10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b51942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb67e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe8396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6441cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0831e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3451b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a090dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb67a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dfc900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39857c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de9f5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb08233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50dabd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31262736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566201d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d94603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1d807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719482ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05451af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71051cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df769f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc0d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841fcdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2adb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a2a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa47693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd997813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc6429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbccc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed829271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cf633a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016de3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ef6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25bb746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (CUDA)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
